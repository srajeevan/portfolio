# Data Analyst

### Education

### Work Experience

Big Data Analyst - Insight Global July 2022 - Present
- Engineered and optimized data processing workflows using Pyspark and Python on the Databricks platform, achieving a 20% reduction in
runtime and a 15% decrease in resource utilization.
- Innovated in writing efficient code for data transformation and analysis, handling large volumes of data with Spark, and integrating Java and
Scala for additional functionality.
- Debugged and improved semi-structured data processing, transforming bronze tables into clean silver tables, and ensuring data quality and
consistency with advanced cleansing techniques.
- Embody a proactive approach to ad-hoc data analysis requests, utilizing Pyspark scripts to extract and analyze relevant data â€¢ for stakeholders.
- Applied data modeling and warehousing concepts to design robust database structures, focusing on scalability and robustness, and
incorporating best practices in data governance.
- Gained intermediate experience in configuring and scheduling data pipelines with Apache Airflow, Glue, and Redshift, and explored
additional distributed data processing frameworks like Apache Flink.
- Implemented a data validation system that caught and resolved 99% of data anomalies, thereby enhancing overall data integrity for critical
decision-making.
- Developed data transformation scripts for Looker and Domo, converting raw data into a structured format for insightful analytics.
- Created interactive dashboards using Looker, engineering views from silver tables and employing LookML to add custom logic tailored to
analytical needs.
- Collaborated with cross-functional teams to understand reporting requirements, translating them into visually appealing and user-friendly
dashboards while incorporating data visualization best practices.
- Worked closely with the data engineering team to deploy and maintain scalable and efficient data structures in a production environment,
leveraging containerization technologies like Docker and orchestration tools like Kubernetes.
- Optimized data storage and retrieval processes by leveraging AWS capabilities, handling data in various formats such as Parquet, JSON, and
Delta, and utilizing AWS components like S3 and Athena.
